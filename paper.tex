% GigaScience template
\documentclass[a4paper,num-refs]{oup-contemporary}

\journal{gigascience}


%%%% Packages %%%%
\usepackage{siunitx}
\usepackage{minted} % Used for JSON highlighting
\usepackage{algpseudocode} % Algorithmic environment
\usepackage{xspace}
\usepackage{booktabs}

%%%%%%

\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{verbatim}
\usepackage{makecell}
\usepackage[flushleft]{threeparttable}

%%%% Commands %%%%
\newcommand{\todo}[1]{\color{red}\textbf{TODO:}#1\color{black}}
\newcommand{\note}[2]{\color{blue}Note: #1\color{black}}
\newcommand{\reprozip}[0]{ReproZip}


\title{NRM-tool (Numerical Reproducibility Measurement): 
a debugging tool to identify numerical errors 
and assess reproducibility of pipelines}

  
  
\begin{document}

\author[1]{Ali Salari}
\author[1]{Lalet Scaria}
\author[2,3]{Gregory Kiar}
\author[2]{Lindsay Lewis}
\author[1]{Tristan Glatard}
\author[2,3]{Alan C. Evans}

\affil[1]{Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada}
\affil[2]{McGill University, Montreal, Canada}
\affil[3]{Montreal Neurological Institute, Montreal, Canada}

\maketitle

\begin{abstract}
Many experiments show that computational analysis results are still not 
completely reproducible. In particular, computational environments 
including different operating systems, hardware categories, and 
software versions are known to have effects on the results produced by 
analysis pipelines. These effects are presumably due to the creation, 
propagation and amplification of small numerical errors across the 
pipelines. 
Although new techniques such as virtualization , version 
control systems , and provenance management tools provide more reliable 
environment and significantly improve the reproducibility of scientific 
findings. 
However, the precise causes of such instabilities and the path along 
which they propagate in the pipelines are unclear.  We present a 
technique to identify the processes in the pipeline that create 
numerical errors along the execution, and we apply this technique to 
the HCP structural pre-processing pipelines.

\end{abstract}

\begin{keywords}
Reproducibility; Numerical Instability; Neuroimaging.
\end{keywords}

\section{Introduction}

A reproducible study provides a context in which one is able to get 
results that are consistent with the original 
work~\cite{plesser2018reproducibility}. Research findings are expected 
to be reproducible so that their authenticity and reliability can be 
evaluated. 
There are different definitions for the term reproducibility, we follow 
Peng's definition~\cite{peng2011reproducible} in which reproducibility 
is defined as the ability to regenerate the same results as the 
original findings when the experiment is reanalyzed given exactly the 
same analytic methods, software package, parameters, and data. 

In addition, numerical reproducibility is defined as the ability to 
regenerate bit for bit identical results from multiple 
runs~\cite{hill2017numerical}. Two files will be considered numerically 
reproducible if they have identical binary contents. Binary comparison 
is calculated by comparing checksums. It must be pointed that a 
computation might be reproducible based on Peng's definition, but not 
be numerically reproducible. For instance, small numerical errors 
created during the pipeline execution may hamper numerical 
reproducibility, but be negligible in the final results.

Recently, scientists began to realize that the results of many 
scientific experiments were not reproducible. Results show that the 
variety of computational infrastructures including workstation types, 
parallelization methods, operating systems, and analysis packages are 
known to influence reproducibility because of the creation of small 
numerical errors~\cite{Gronenschild2012, diethelm2012limits, 
Glatard2015, bowring2018exploring}. The propagation and amplification 
of these tiny errors by analysis pipelines may cause reproducibility 
issues. 
In this case, the analysis pipelines are said to be numerically 
unstable. Numerical instability is a characteristic of the pipelines 
which amplify small numerical errors and then hamper the 
reproducibility of the analyses depending on the length of the pipeline 
and magnitude of the errors. In many cases, numerical instability is an 
important issue for reproducibility.
 
In particular, the effect of operating system is quantified 
in~\cite{Glatard2015, Gronenschild2012} more specifically on some of 
the main neuroimaging pipelines including FSL, CIVET and Freesurfer 
pipelines. The results of comparisons indicate a significant errors 
between analysis results of the same experiment on different operating 
system. It should be noted that \emph{error} is defined as the binary 
differences between results of two analyses in this paper, which is 
calculated by the checksum method. 
The operating system is not the only part of the computational 
environment that may hamper reproducibility. Many other computational 
factors including hardware configuration, software version, and 
parallelization techniques are reported as the main influential 
computing infrastructure on reproducibility. 

There are different solutions to make reproducible analysis including 
containerization techniques to encapsulate software/hardware 
dependencies, provenance capturing tools, and version control systems. 
However, a comprehensive solution requires to fix the numerical 
instabilities. For this purpose, we introduce a Numerical 
Reproducibility Measurement (NRM) tool to identify the processes in 
pipeline that create numerical errors across different runs. In this 
paper, the execution results of pipelines across different operating 
systems are considered to find out the cause of the irreproducibility 
using an interposition techniques.

The reminder of this paper describe the NRM-tool in different parts as 
the way of capturing provenance information and their representation, 
the influence of different type of subjects, and how to classify 
processes and characterize errors. We describe the HCP preprocessing 
pipelines as a popular neuroimaging pipelines to test and apply the 
NRM-tool on. Finally, we report the experimental results, the processes 
that are responsible for the errors in the pipelines.


\section{Tool description}

We present a Numerical Reproducibility Measurement tool (NRM-tool) to 
measure numerical reproducibility of the pipelines across operating 
systems automatically (see Figure~\ref{fig:overview}). We build the 
containerized version of pipelines for each operating system using 
docker technology which encapsulated whole the pipeline dependencies. 
The docker images consists of an specific computing environment such as 
operating system version, hardware configuration, software version, 
etc. Furthermore, to simplify computational reproducibility of the 
analysis, we execute docker images using Boutiques framework based on 
the command-line descriptions~\cite{glatard2017boutiques}. The 
Boutiques descriptor specifies the parameters and implementation of the 
experiment, and the invocation schema describes the parameter values 
such as input/output data path.

NRM-tool produces the error JSON schema in which recognizes output files 
with errors using the checksum comparison of the results. 
We consider that a file contains errors if 
its binary comparison is different, otherwise file has no error. Also, 
we use \reprozip tool to capture the provenance information. 
Subsequently, using a iterative approach, we classify processes into 
different categories based on the state of read/write files 
with/without error. NRM-tool identifies relevant processes in the 
pipeline creating, propagating, or removing errors. We will explain 
each steps in detail in the following sections.

\begin{figure}
\centering
  \includegraphics[width=\columnwidth]{images/overview.png}
  \caption{An overview of the proposed technique.}
  \label{fig:overview}
\end{figure}


\subsection{Provenance capture and representation}

We use the \reprozip tool~\cite{Chirigati2016} to record: (1) 
the tree of processes executed by the pipeline and (2) the list of 
files read and written by each process. This information is collected 
by system call interceptions, through the \texttt{ptrace} Unix system 
call, and stored in a \texttt{SQLite} database. The database stores 
information on all the processes which are created by the 
\texttt{clone()} or \texttt{fork()} system calls. It also records the 
files opened by each process (in read, write or execution mode), 
including the result files and the temporary files.

NRM-tool reconstructs the tree of processes starting from the first process 
created by the pipeline and identifying its child processes as the ones 
that were created through \texttt{clone()} or \texttt{fork()}. It 
creates a process graph from this tree by adding edges corresponding to 
file dependencies between processes. A file dependency is defined 
between processes A and B if a file written by A is read by B. 
Figure~\ref{fig:simple_script} shows an example of a process graph 
constructed for the simple bash script illustrated in 
Algorithm~\ref{algo:sample-script} from the brain extraction procedure.


\begin{algorithm}[h!]
\caption{Sample script from brain extraction process}
\label{algo:sample-script}
\begin{verbatim}
#!/bin/bash
if [ # !=1 ]
then
    echo "usage: 0 <inputimage.nii.gz>"
    exit 1
fi
input_image=$1
bet_output="$(basename ${input_image} 
                       .nii.gz)_brain.nii.gz"
bet_output_binarized="$(basename ${input_image} 
                       .nii.gz)_brain_bin.nii.gz"

bet ${input_image} ${bet_output} > bet_temp.out
echo "Voxels / Volume in brain mask:"
fslstats ${bet_output} -V
fslmaths ${bet_output} ${bet_output_binarized}
echo "Voxels / Volum in binarized brain mask:"
\rm bet_temp.out
\end{verbatim}
\end{algorithm}

\begin{figure}
\centering
  \includegraphics[scale=0.3]{images/simple_graph}
  \caption{Process graph constructed from an example brain extraction
	script. Every node in the graph is labeled using (1) a process id 
	created by our reconstruction, (2) the name of the executable run 
	by the process. Processes that read or write temporary files are 
	represented with squares; other ones with circles. Plain edges 
	represent the process tree (\texttt{fork()} or \texttt{clone()} 
	system calls). Dashed edges represent file dependencies: temporary 
	files are in yellow and result files are in green.}
  \label{fig:simple_script}
\end{figure}

\subsection{Subject type}

Depending on the variety of computing environments, the produced 
process graph can be different. In particular, different analysis 
software package produce different process graphs for the similar 
experiments because of following different methods. We used a fixed 
analysis software package in different conditions, however, we found 
that different subjects may produce a different process graph. Among 
our dataset, we identified 4 types of subjects with different numbers 
of T1w and T2w images. 

In addition, we verified that the process graphs were identical for all 
subjects of the same type in different versions of CentOS operating 
system. The remainder of the analysis is done separately for each 
subject type.

\subsection{Graph analysis}

NRM-tool classifies the processes into four 
categories based on the process graph and the error JSON file 
mentioned in the previous sections:
\begin{enumerate}
\item Processes that read files that do not have errors and write files 
that do not have errors are \emph{transparent} (Figure~\ref{fig:processes}.a).
\item Processes that read files 
that do not have any error but write files that have errors 
\emph{create} errors in the pipeline (Figure~\ref{fig:processes}.b).
\item Processes that read files 
that have errors and write files that do not have errors \emph{remove} 
errors from the pipeline (Figure~\ref{fig:processes}.c).
\item Processes that read files that have errors and write files that also have errors are 
\emph{unknown} (Figure~\ref{fig:processes}.d).
\end{enumerate}

\begin{figure}%\centering
\centering
    \begin{subfigure}{0.4\linewidth}
        \includegraphics[scale=0.34]{images/green.png}
        \caption{Transparent}
        \label{fig:green}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.4\linewidth}
    \includegraphics[scale=0.34]{images/red.png}
    \caption{Creates error}
    \label{fig:red}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.4\linewidth}
    \includegraphics[scale=0.34]{images/blue.png}
    \caption{Removes error}
    \label{fig:blue}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.4\linewidth}
    \includegraphics[scale=0.34]{images/yellow.png}
    \caption{Unknown}
    \label{fig:yellow}
\end{subfigure}
    \caption{Different type of classified process based on the input/output files.
  Dashed edges refer to the file dependencies between processes A and B 
  if a file written by A is read by B. Solid black edges refer to the 
  relationship between parent and child processes.}
    \label{fig:processes}
\end{figure}

There are some issues that should be addressed to classify process 
properly, including capturing temporary files that have been removed 
during the execution, capturing files written by multiple processes, 
and unknown process.

\subsubsection{Capture temporary files} 
The classification of processes that read or 
write temporary files is uncertain when these files are deleted during 
the execution. To address this issue, we replace every process P that 
writes temporary files with a modification of P that first calls P and 
then backs up all its output files to a read-only directory. This 
replacement is done by modifying the Unix PATH variable to point to a 
directory containing the modified versions of the processes. This 
solution does not cover the temporary files that are removed by P 
itself; this is not a problem since these files do not play any role in 
the subsequent steps of the pipeline, by definition. 

\subsubsection{Capture multiple write files} 
Files written by multiple processes also lead 
to unknown classification labels. To address this issue for a file F 
written by processes in \textbf{P} = \{$P_{1}$, \ldots $P_{n}$\}, we 
(1) check that processes in \textbf{P} do not write concurrently to F, 
(2) we establish an order on \textbf{P} based on the creation timestamp 
of the processes, (3) we replace ever process $P_{i}$ in \textbf{P} by 
a wrapper that first calls $P_{i}$ and then backs up F to a read-only 
directory. Thus, multiple versions of F are saved and used in the 
analysis. 

Furthermore, cycles may be present in the process graph in case a file 
was written by more than one process. We remove such cycles by removing 
file edges between processes A and B when A's process creation 
timestamp is posterior to B's or when A=B. Indeed, such edges cannot 
happen in practice unless A and B were running concurrently, which we 
assume is not the case (we also checked that on the workflow we 
tested). 

\subsubsection{Unknown process} 
Another challenge is to classify processes that 
read files that have errors and write files that also have errors. In 
such situations, it is not possible to determine from the pipeline 
results whether the process created errors, removed some errors, or was 
transparent. We label such processes \emph{unknown}.
To address this issue, we developed an iterative approach that 
consists of the following steps: 

\begin{enumerate}
  \item Run the pipeline in conditions 1 and 2; classify the
    processes as \emph{transparent}, \emph{create errors},
    \emph{remove errors} or \emph{unknown}.
  \item If there are \emph{unknown} processes then: replace each 
  process P that create errors by a process Q that copies the results 
  produced by process P in condition 1 to the pipeline output in 
  condition 2. 
  \item Repeat steps 1 and 2 until there is no \emph{unknown} process left.
\end{enumerate}

The replacement of process P at step 2 is done by replacing P with a 
custom script produced by step 1. This custom script copies the results 
obtained in condition 1 if it is invoked with the arguments of a 
process that created errors, and calls P's original executable 
otherwise. The replacement is done through the PATH variable, as 
before. This algorithm converges to a process graph without any 
\emph{unknown} process after a finite number of iterations.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{images/iterative_modif}
  \caption{iteration of proposed modification.}
  \label{fig:iterations}
\end{figure}

Figure~\ref{fig:iterations} illustrates our iterative classification 
process for the example in Figure~\ref{fig:simple_script}. At every 
step, processes that created errors are shown in red, processes that 
removed errors are in blue, processes that propagated errors are in 
yellow, and other processes (transparent processes) are in green. As in 
Figure~\ref{fig:simple_script}, plain black edges represent the process 
tree and dashed edges represent file dependencies: green edges 
represent files with no errors, while red edges represent files with 
errors. Temporary files are not represented because they have been 
backed up as previously described.

The three steps in Figure~\ref{fig:iterations} correspond to the 
iterations of the classification algorithm. At step one, \texttt{2\#bet2} 
is classified as error creator (red) as it produced files with errors 
from files without error. \texttt{3\#fslstats} is classified as error 
remover (blue) as it produced files without errors from files with 
errors. \texttt{4\#fslmaths} and \texttt{5\#fslstats} are classified as 
unknown (yellow) as they produce files with errors from files with 
errors.

At step 2, the files produced by \texttt{2\#bet2} in the tested 
condition are replaced with the files produced by \texttt{2\#bet2} in 
the other condition. \texttt{3\#fslstats} is now classified as 
transparent, \texttt{4\#fslmaths} is now classified as error creator 
and \texttt{5\#fslstats} is still unknown.

At step 3, the files produced by \texttt{4\#fslmaths} in the tested 
condition are replaced with the files produced by \texttt{4\#fslmaths} 
in the other condition. \texttt{5\#fslstats} is now transparent.
 
As a result of those 3 steps, the final process classification is: 
\texttt{2\#bet2} and \texttt{4\#fslmath} are error creators (red), 
\texttt{3\#fslstats} is error remover (blue) and the other processes 
are transparent.


\section{Experiments}
\subsection{HCP pipelines and datasets}
\subsection{Results}

\section{Discussion}

\note{Pipeline amplify small numerical differences because they are numerically 
unstable. Furthermore, math libraries evolve over time, leading to 
different numerical errors. we listed some of the irreproducibility 
causes of the pipelines as we mentioned in the previous section 
overally along with exact command arguments.

Mention that this was only possible because the unprocessed data was 
shared in the first place. DICOM to Nifti conversion was out of scope 
and may introduce other issues.}

\section{Conclusion}

\note{Our technique is able to characterize the stability of a pipeline's 
components automatically. The numerical instability in the 
PreFreesurfer HCP pipeline arises mainly from linear and non-linear 
registration processes implemented in FSL FLIRT and FNIRT. 

There are a few ways to impede such instabilities:
\begin{itemize}
\item Use a single operating system
\item Containerize pipelines
\item Increase numerical precision
\item Be stricter on truncation and rounding standards (IEEE 754)
\item Build static executable
\end{itemize}

The results still suffer from small perturbations literally because of 
the fact that pipeline are not numerically stable. The preferred 
solution is to detect and fix numerical instability of the pipeline 
instead of masking the problem. These processes need to be reviewed to 
understand and correct the cause of instabilities. }

\section{Acknowledgments}

CBRAIN team. Compute Canada(Calcul Quebec).

\bibliographystyle{plain}
\bibliography{biblio}


\end{document}
