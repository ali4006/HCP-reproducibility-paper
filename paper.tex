% GigaScience template
\documentclass[a4paper,num-refs]{oup-contemporary}

\journal{gigascience}


%%%% Packages %%%%
\usepackage{siunitx}
\usepackage{minted} % Used for JSON highlighting
\usepackage{algpseudocode} % Algorithmic environment
\usepackage{xspace}
\usepackage{booktabs}

%%%%%%

\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{verbatim}
\usepackage{makecell}
\usepackage[flushleft]{threeparttable}

\usepackage{subcaption}
\usepackage{xspace}
\usepackage{stmaryrd} % for llbracket and rrbracket
\usepackage{amsmath}
\usepackage{ulem}
\usepackage{pifont}

\hypersetup{
colorlinks=true,% hyperlinks will be coloured
linkcolor=blue,% hyperlink text will be blue
linkbordercolor=blue,% hyperlink border will be blue
}
\makeatletter
\Hy@AtBeginDocument{%
  \def\@pdfborder{0 0 1}% Overrides border definition set with colorlinks=true
  \def\@pdfborderstyle{/S/U/W 1}% Overrides border style set with colorlinks=true
                                % Hyperlink border style will be underline of width 1pt
}
\makeatother

\DeclareMathOperator*{\argmin}{argmin}



%%%% Commands %%%%
\newcommand{\todo}[1]{\color{red}\textbf{TODO:}#1\color{black}}
\newcommand{\note}[2]{\color{blue}Note: #1\color{black}}
\newcommand{\reprozip}[0]{ReproZip\xspace}
\newcommand{\tristan}[1]{\color{blue}\textbf{From Tristan:}#1\color{black}}
\newcommand{\english}[1]{\uwave{#1}}
\newcommand{\toolname}[0]{Spot\xspace}
 
\title{File-based localization of numerical perturbations in data analysis pipelines}
\begin{document}

\author[1]{Ali Salari}
\author[2,3]{Gregory Kiar}
\author[2]{Lindsay Lewis}
\author[2,3]{Alan C. Evans}
\author[1]{Tristan Glatard}

\affil[1]{Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada}
\affil[2]{McGill University, Montreal, Canada}
\affil[3]{Montreal Neurological Institute, Montreal, Canada}



\maketitle

\begin{abstract} 

Data analysis pipelines are known to be impacted by operating system
updates and other infrastructural factors, presumably due to the creation,
propagation and amplification of numerical errors. However, the precise
causes of such instabilities and the path along which they propagate in the
pipelines are unclear, while this process could play a major role in the
current reproducibility crisis. We present a technique to identify the
computational processes in a pipeline that create numerical differences
when executed in different computational conditions. Our technique
leverages system-call interception to reconstruct processing and file
graphs without pipeline instrumentation. By applying our tool to the
structural pre-processing pipelines released by the Human Connectome
Project, we find that linear and non-linear registration are the cause of
most numerical instabilities in such pipelines, which confirms previous
findings. 

\end{abstract}

\begin{keywords}
Computational reproducibility; Numerical instability; Neuroimaging.
\end{keywords}


\section{Introduction}

% Reproducibility: crisis, numerical stability
% Containerization isn't the solution

Numerical stability is believed to play a central role in the differences
observed in data analysis pipelines across computational infrastructures.
Variability resulting from operating system upgrades, parallelization, or
even minor data perturbations may originate in errors that accumulate in
numerical computations, with important consequences on the reproducibility
of data analysis results. However, it remains challenging to identify the
exact elements in a complex pipeline that are prone to numerical instability,
leaving pipeline developers and users with the daunting task to manually
compare results obtained in different conditions. 

In neuroimaging, our primary application field, data analyses often consist
of hundreds of computational processes, sometimes coming from multiple
toolboxes, that are aggregated to perform a specific function. For
instance, the fMRIprep pipeline~\cite{esteban2019fmriprep} assembles software blocks
from FSL~\cite{jenkinson2012fsl}, AFNI~\cite{cox2012afni}, Freesurfer~\cite{fischl2012freesurfer} and ANTs~\cite{avants2009advanced} to provide a state-of-the art
functional MRI processing tool with minimal user input. Another example are the pipelines of the Human
Connectome Project~\cite{glasser2013} that combine tools from FSL and Freesurfer to pre-process
structural, functional and diffusion data from their
open dataset. In both cases, pipelines leverage toolboxes that are
widely trusted in the community. At the same time, substantial results
variations have been observed in these toolboxes resulting from minor data
or infrastructure perturbations~\cite{Glatard2015, Gronenschild2012, Lewis2017-ll}, which suggests that further investigation of their
numerical conditioning is required. For such complex pipelines, a
lightweight solution has to be found to perform such evaluations with
limited code instrumentation.

Numerical evaluations are traditionnally performed using techniques such as
interval arithmetics~\cite{hickey2001interval} that require complete code re-writes and are
therefore barely applicable to complex pipelines. Recently, Monte-Carlo
Arithmetic~\cite{Parker1997-qq, Denis2016-wo} provided a practical way to
evaluate the uncertainty of numerical results without the need to rewrite
the application in a different paradigm. By perturbating floating-point
computations, it introduces a controllable amount of noise in the
pipelines, effectively sampling results from a random distribution. While
this technique is very appealing, it suffers from two main issues that make
it impractical at the scale of a complete pipeline. First, it requires that
all software components be recompiled for MCA instrumentation, which is not
always feasible. Second, it multiplies the execution time by a factor of 10
to 100, which is impractical when executions already take a few hours to
complete. 

We present Spot, a tool to identify the source of numerical differences in
complex pipelines without instrumentation. Using system-call interception
through the Reprozip tool~\cite{chirigati2016reprozip}, Spot traverses graphs of processes and
intermediary files to pinpoint the pipeline components that are unstable
across execution conditions. When differences start accumulating,
effectively masking any further instability, it restores clean data copies
through a set of wrapper scripts. Wrapper scripts are also used to restore
temporary data that might have been deleted during the execution, and to
disambiguate files that have been written by multiple processes. The remainder of this paper 
presents the design of our tool, and its 
application to pre-processing pipelines of the HCP project. 

\todo{explain the issue with propagating differences and with transient files}

% % Verificarlo and MCA, require recompilation
% Numerical instabilities originate in the limited \todo{precision} of floating-point representations. 
% Traditionnaly, it has been evaluated through 

% % -> File-based analysis of pipelines without instrumentation
% % Reprozip

% Reproducibility is a crucial element of the scientific works, 
% as it enables researchers to evaluate authenticity and reliability 
% of the findings~\cite{plesser2018reproducibility}.
% Reproducibility is defined as the ability to regenerate the same 
% results as the original findings when the experiment is reanalyzed by 
% the same analytic methods, software package, parameters, and 
% data~\cite{peng2011reproducible}. 
% In addition, numerical reproducibility is defined as the ability to 
% regenerate bit for bit identical results from multiple 
% runs~\cite{hill2017numerical}. 
% We considered the numerical reproducibility in our experiments by comparing 
% the binary content of the results using the checksum method.

% Recently, validation of the reproducibility has been widely investigated 
% in the field of neuroimaging in which uses optimized 
% processing methods with the purpose of functional and structural 
% assessments of the human brain.
% According to the previous 
% studies, the variety of computing infrastructures including workstation 
% types, parallelization methods, operating systems, and analysis 
% packages are known to influence the reproducibility of the analyses~\cite{Gronenschild2012, 
% diethelm2012limits, Glatard2015, bowring2019exploring}.
% These irreproducibility issues are reported as the result of the 
% creation, propagation, and amplification of small numerical 
% differences.

% In particular, the effect of the operating systems on  
% computational pipelines shows the creation of small numerical differences~\cite{Glatard2015, Scaria2017}.
% These differences mainly correspond to the mathematical functions implemented 
% in different operating system libraries.
% For instance, changing the mathematical functions like \emph{expf()} and 
% \emph{cosf()} which manipulate the precision of floating-point representations, 
% between \emph{glibc} libraries in different operating systems can produce 
% small numerical differences.
% Besides, a similar issue is expected for any operating system which is 
% based on \emph{glibc}, the GNU C library.

% There are different approaches to improve the reproducibility of the analysis, 
% including containerization techniques that encapsulate software/hardware dependencies, 
% provenance capturing tools, and version control systems. 
% However, a comprehensive solution requires to fix the numerical instabilities instead of 
% masking the problem. For this purpose, bagging is one solution that has been used to 
% stabilize motion estimation analysis in fMRI~\cite{Glatard2018hbm}. 
% Bootstrap aggregation reduces the instabilities, but it needs expensive computations.
% As the second solution, a debugging tool can help to identify and 
% fix the instabilities in the pipeline. Therefore, we introduce 
% the numerical reproducibility measurement tool named \emph{Spot} to identify 
% the processes in the pipeline that create numerical differences across different conditions. 
% Furthermore, we can stabilize the identified processes in the next steps.
\todo{explain creation vs propagation}
\todo{define pipeline}

\section{Tool description}

\toolname identifies the processes in a pipeline that produce different
results when executed in two different conditions. First, a directed
bipartite provenance graph is recorded for each subject, where nodes
represent application processes and files, and edges represent read and
write file accesses (Step 1 in Figure~\ref{fig:overview-tool}). Second,
transient files, i.e., files that are either deleted during pipeline
execution or modified by multiple processes, are identified (Step 2) and
disambiguated, resulting in a provenance \emph{DAG} (Directed Acyclic
Graph) in which file nodes have a single parent (in-degree of 1) (Step 3)
\tristan{Ali, this is not true in Figure 1: at step 3 there are still
transient files}. DAGs produced in the two conditions are then compared, in
a step-by-step execution that prevents the propagation of differences in the pipeline (Step
4). Finally, a labeled graph is produced that identifies the
non-reproducible processes in the pipeline. 


To ensure that a file can be unambiguously associated with the process that
created it, we assume that pipeline processes can be re-ordered such that:
\begin{enumerate}
\item Processes don't run concurrently;
\item Each process sequentially reads, computes, and writes. 
\end{enumerate}
In practice, pipeline processes may still run concurrently provided that
they don't write concurrently to the same files. A process may also
interleave file writes with computing, for instance when different file
blocks are processed sequentially. However, only a single version of the
file must eventually be made available to the other processes. Similarly,
in case processes write and delete temporary files throughout their
execution, these temporary files must not be read by any other process in
the pipeline.

In addition, we also require that processes are
associated to a command line (executable and arguments), to facilitate
process instrumentation.

\begin{figure*}
  \centering
    \includegraphics[width=.8\textwidth]{images/spot-diagram}
    \caption{\toolname overview.}
    \label{fig:overview-tool}
  \end{figure*}

\subsection{Recording provenance graphs}

We use the \reprozip tool~\cite{rampin2016reprozip}
to capture: (1) the set of processes created by the
pipeline, and
(2) the set of files read and written by each process, including
temporary files. \reprozip collects this information through the
\texttt{ptrace()} system call, with no required instrumentation of the pipeline.
Using the \reprozip trace, \toolname reconstructs a provenance graph by creating process and file
nodes and by adding directed edges corresponding
to file reads and writes (Figure~\ref{fig:provenance-graph}).

Provenance graphs are often subject-dependent, due to variations in input data among subjects.
%, which justifies the processing
% of multiple subjects. For instance, data acquisitions may be
% repeated a different number of times in each subject, for noise-reduction
% purposes. 
Some of these differences can be neglected, for instance when a
data decompression step is present at the beginning of the execution for
some subjects only, but other differences cannot, for instance
when complete different processing paths are used in different subjects. \toolname 
includes helpers to identify different instances of provenance graphs \tristan{mention tree distance here}.

\begin{listing}
  \inputminted{bash}{"bin/example.sh"}
  \caption{Example pipeline}
  \label{listing:sample-script}
\end{listing}

\begin{figure}
\begin{subfigure}{0.4\linewidth}
  [ FIGURE GOES HERE ]

  \caption{Raw provenance graph (\reprozip output), with transient files shown in gray.}
  \label{fig:provenance-graph}
\end{subfigure}
\hfill
\begin{subfigure}{0.4\linewidth}
  [ FIGURE GOES HERE ]

  \caption{Provenance DAG, with disambiguated transient files.}
  \label{fig:provenance-dag}
\end{subfigure}
  % \includegraphics[width=.8\columnwidth]{images/provenance-graphs}
  \caption{Provenance graphs
  created from the example pipeline in
  Listing~\ref{listing:sample-script}.
  Processes are represented with circles, files with rectangles, and read/write accesses 
  with plain edges. 
  For visualization purposes, the process tree is represented too, with
  dashed edges. Every node in the graph is labeled using (1) a process id
  created by our reconstruction, (2) the name of the executable run by the
  process or the file name. Process 0 is the initial call to the
  \texttt{sh} interpreter; the other processes are the calls to FSL bet,
  stats and maths made in Listing~\ref{listing:sample-script}. Process 73
  is forked by process 2: it was captured by \reprozip while it did not
  appear in Listing~\ref{listing:sample-script}.
}
  \label{fig:simple_script}
\end{figure}

\subsection{Capturing transient files}

We capture temporary files by replacing every
process P by a wrapper that first calls P and then saves the produced
temporary files to a read-only directory. This process replacement is done by pre-pending
 to the \texttt{PATH} environment
variable a directory that contains a wrapper script named after the executable
called by P.

Files written by multiple processes are disambiguated using a similar technique. For a
 file F written by the processes in \textbf{P} = \{$P_{1}$, \ldots,
 $P_{n}$\}, we first check that processes in \textbf{P} do not
 write concurrently to F, which would violate our assumptions. Then, we
 replace every process $P_{i}$ by a \texttt{PATH}-based wrapper that first
 calls $P_{i}$ and then saves F to a read-only directory. In this way,
 successive versions of F are preserved for comparison. We finally
 update the provenance graph accordingly, so that all files in the graph
 have an in-degree of 1 (Figure~\ref{fig:provenance-dag}). This operation also makes the provenance graph
 acyclic, since we assumed that a process could only release a single version of a file.



\subsection{Labelling processes} 

After capturing transient files, we re-run the pipeline
step by step in Condition 2 to label processes. The output files
produced by a process in Condition 2 are compared to the ones produced
in Condition 1. If no differences are found, the process is marked as
reproducible. Otherwise, the process is marked as non-reproducible and the
output files produced in Condition 1 are copied to Condition 2, to ensure
that differences do not propagate further in the pipeline. Processes are
instrumented transparently, through a modification of the \texttt{PATH}
variable similar to the one described previously. By default, differences
in output files are identified by comparing file checksums. Other
comparison functions can also be defined for specific file types, for
instance to ignore file headers or file sections containing timestamps.
\toolname finally creates a labeled
provenance graph highlighting non-reproducible processes.

Figure~\ref{fig:iterations} illustrates the incremental labelling 
process for the example in Figure~\ref{fig:simple_script}. 
At step 1, process \texttt{73\#bet2} is labeled as non-reproducible (red) 
as it produces files with differences. Subsequently, the files produced by \texttt{73\#bet2} in  
Condition 2 are replaced with the files produced by \texttt{73\#bet2} in 
Condition 1.
At step 2, process \texttt{75\#fslmaths} is executed and then labeled 
as reproducible (green) as it produced files without differences.
At step 3, \texttt{76\#fslstats} is also labeled as 
reproducible as it didn't produce any file.

\begin{figure}
  \centering
  \includegraphics[width=.8\columnwidth]{images/labelling-process}
  \caption{Illustration of the labelling approach. Non-reproducible processes are shown in red.}
  \label{fig:iterations}
\end{figure}



\section{Experiments}

We applied the \toolname to find the origin of between-OS differences that
frequently occur in neuroimaging pipelines. In particular, we studied the minimal
pre-processing pipelines released by the Human Connectome Project
(\href{https://www.humanconnectome.org}{HCP}), a leading initiative in
neurosciences. 

% In this experiment, we analyse the numerical reproducibility of computational pipelines 
% and identify the origin of differences across the operating systems. 
% Two types of differences can occur in the subjects due to the differences
% in the operating systems. One is between-OS differences caused by the
% operating system library updates and the other type, within-OS differences
% occur as a result of the pseudo-random processes used in the pipelines.

% In particular, Spot tool is tested on the neuroimaging applications which are 
% predominantly using mathematical libraries. Therefore, we expect to find 
% differences as a result of changing the mathematical functions between operating system libraries.

% This section describes datasets and pipelines used for the analyses, and the way of data processing.

\subsection{HCP pipelines and dataset}

% We used our tool to evaluate the reproducibility of the pipelines from the Human Connectome 
% Project .
% The HCP initiative is an effort to acquire and analyse 
% brain connectivity data from 1200 healthy adults.
% It enables the neuroscience 
% research community to discover relationships between brain circuits and 
% individual behaviors. This helps to understand a wide range of brain disorders.
% The HCP project provides database services (ConnectomeDB) for storing and 
% sharing primary and processed data freely, and data analysis pipelines that 
% are available under an open-source license.

The HCP developed a set of pre-processing pipelines to process structural,
functional, and diffusion MRI data acquired in the project. We focus on HCP
pre-processing pipelines for structural data, and particularly
on PreFreeSurfer and FreeSurfer. 
A detailed description of the analyses done by these
pipelines is available in~\cite{glasser2013}. 
In summary, PreFreeSurfer consists of the following steps: 
\begin{itemize}
\item Distortion Correction (DC), 
\item Anatomical Average (AAve), 
\item Anterior/Posterior Commissure Alignment (ACPC-A), 
\item Brain Extraction (BExt), 
\item Bias Field Correction (BFC), 
\item Atlas-Registration (AR).
\end{itemize}
And FreeSurfer consists of the following ones: \tristan{Check these steps}
\begin{itemize}
\item Image downsampling, 
\item T1w image registration, 
\item Surface placement, 
\item Surface registration.
\end{itemize}
The average processing time per subject is approximately 2~hours for
PreFreeSurfer and 8~hours for FreeSurfer. The average output file size is
approximately 2.7~GB for PreFreeSurfer and 4.1~GB for FreeSurfer.
%\tristan{Ali, are these numbers per subject or for 20 subjects? it's an average of 20 subjects}

We randomly selected 20 unprocessed subjects 
from the HCP data release S500 
available in \href{https://db.humanconnectome.org}{the ConnectomDB repository}. 
For each subject, available data consisted of 1 or 2 T1 images, and 1 or 2 T2 images. 
% \tristan{Can you let me know where the data is located, so that I could describe it here?
% Subjects are located in consider:/data/asalari/ali-tests/PFS\_Centos6\_Traced
% and consider:/data/asalari/ali-tests/PFS\_Centos7\_Traced 
% and outputs are in consider:/data/asalari/ali-tests/nurm-out}
Acquisition protocols and parameters are detailed in~\cite{van2013wu}. \tristan{Add summary of matrix sizes.}

% HCP data collected from different types of imaging techniques, including 
% structural imaging (sMRI), functional imaging (fMRI), and diffusion imaging (dMRI).
% The structural images include T1-weighted (T1w) and T2-weighted (T2w) images, which 
% help in the diagnosis of brain injury.
% The images are indexed with a suffix if several scans of the same modality were acquired.
% For example, some data may include two T1w images with low and high resolutions.
% The functional images, including task-based fMRI and resting-state scans, 
% enable the measurement of functional activations within brain areas. 
% Diffusion imaging is another kind of MRI technique, which measures 
% the anatomical connectivity between regions.

\subsection{Data processing}

We built Docker images for the HCP pre-processing pipelines v3.19.0
(PreFreeSurfer and FreeSurfer) in CentOS 6.9 (Final) and CentOS 7.4 (Core), available on
\href{https://hub.docker.com/r/bigdatalabteam/hcp-prefreesurfer/}{DockerHub}.
Container images contain the HCP software dependencies, including FSL
(version 5.0.6), FreeSurfer (version 5.3.0-HCP, CentOS4 build), and
Connectome Workbench (version 1.0).

We processed the 20 subjects with PreFreeSurfer and FreeSurfer, using the 2
CentOS versions. Each subject was processed twice on the same operating
system to detect within-OS variability coming from pseudo-random
operations. We ignored execution-specific information such as file path or
timestamps by using FreeSurfer tools \texttt{mri\_diff},
\texttt{mris\_diff}, and \texttt{lta\_diff} that determine if MRI volumes
or transformations differ. To compare segmentations, we used the Dice coefficient defined as follows:
\[DICE=\frac{2|X \cap Y|}{|X| + |Y|}\].

% Finally, to cluster the subjects, the threshold value is set to zero in the clustering method. 
% Also, we used the nearest neighbor algorithm to calculate the distance between clusters.


\subsection{Results}

% \subsection{Subject clustering}

% To reduce the number of provenance graphs to be examined, we cluster
% process trees using agglomerative hierarchical clustering, as implemented
% in SciPy~\cite{oliphant2007scipy}. We use the tree edit
% distance~\cite{zhang1989simple} between process trees, as implemented in
% the zss Python package. This distance is defined as the minimum number of
% edit operations to transform one tree into the other. Three edit operations
% are considered: node label modification, node removal, and node insertion.
% Each operation has an associated cost of 1. 
% //If threshold is 0, why do we even need the edit distance?//


\subsubsection{Within-OS differences}

We did not observe any within-OS difference in PreFreeSurfer. In
FreeSurfer, we identified 2 processes leading to within-OS differences due
to the use of pseudo-random numbers: image registration with
\texttt{mri\_segreg}, and cortical surface curvature estimations with
\texttt{mris\_curvature}. To remove these differences, we fixed the random
seed used by FreeSurfer using the \texttt{--seed} command-line option. 

\subsubsection{Between-OS differences in PreFreeSurfer}
%\subsubsection{PreFreeSurfer pipeline analysis} 

We identified four types of subjects with different PreFreeSurfer
provenance graphs (Table~\ref{table:data-clusters}). Differences between
subject types are coming from different  numbers of T1 and T2 images in the
raw data: subjects of type 1 have 2 T1 images and 2 T2 images, while
subjects of type 2 only have 1 T1 image and 1 T2 image. We verified that
the provenance graphs were identical for all subjects of the same type, for
both versions of CentOS.

\begin{table}
\centering
\begin{threeparttable}
\caption{Types of provenance graphs in PreFreeSurfer.}
\label{table:data-clusters}

\begin{tabular}{cccccc}
\toprule
       &                        &  \multicolumn{4}{c}{Image modalities in subject data}    \\ 
\cmidrule(lr){3-6}       
Type   &   \makecell{Number of \\ Subjects}   &  T1w\_1          & T1w\_2      & T2w\_1          & T2w\_2        \\ \midrule
1      &               9                      &   \ding{51}      &   \ding{51} &   \ding{51}     &   \ding{51}   \\ 
2      &               8                      &   \ding{51}      &             &   \ding{51}     &               \\ 
3      &               1                      &   \ding{51}      &             &   \ding{51}     &   \ding{51}   \\ 
4      &               2                      &   \ding{51}      &   \ding{51} &   \ding{51}     &               \\ 
\bottomrule
\end{tabular}
\end{threeparttable}
\end{table}


Figure~\ref{fig:pfs_freq} shows the frequency of non-reproducible pipeline processes
in PreFreeSurfer. 
Differences were observed in linear registration 
with FSL FLIRT (in ACPC-Alignment, Brain Extraction, Distortion Correction, and
Atlas Registration), in non-linear registration with FSL FNIRT (in Brain Extraction 
and Atlas Registration), and in image warping with FSL \texttt{new\_invwarp} (in Brain Extraction 
and Atlas Registration). Differences were also observed in image mean 
computations with FSL maths  (in Anatomical Average). \tristan{figure and caption to be fixed.} 

\tristan{Ali, it would be useful to show a complete PreFreesurfer graph here.}

% Between subject differences can be a result of different subject types. 
% For instance, the subjects contain two T1 images need more calculations 
% to produce an average image between them.


% make figure in two columns use {figure*}
\begin{figure*}
\centering
  \includegraphics[width=\textwidth]{images/pfs_heatmap.png}
  \caption{Heatmap of non-reproducible processes across PreFreeSurfer pipeline steps (N=20). 
  \tristan{Remove frequency values as they are shown by the color already. Remove multipliers, multiply cells instead. Cells should just contain subject numbers.}
  Each cell represents the occurrence of a particular command line in a
  pipeline step among Anatomical Average (AAve), Anterior/Posterior
  Commissure Alignment (ACPC-A), Brain Extraction (BExt), Bias Field
  Correction (BFC), or Atlas-Registration (AR). Cell labels indicate the
  number of subjects where the corresponding process appears. For example,
  the FLIRT tool was invoked 6 times in step DC for each of the 20
  subjects: 2 instances weren't reproducible in 95\% of the subjects, 3
  instances were always reproducible, and 1 instance wasn't reproducible in
  85\% of the subjects.
        }
  \label{fig:pfs_freq}
\end{figure*}


To visually illustrate the observed differences,
Figure~\ref{fig:fnirt_result} compares FNIRT results in Brain Extraction
for a particular subject. Differences appear to be very substantial, in
particular in the frontal and parietal lobes. 
\begin{figure}
  \centering
    \includegraphics[width=\columnwidth]{images/t2w_alignment.png} 
    \caption{Differences between T2 FNIRT results in PreFreeSurfer's Brain Extraction (CentOS6 vs 
    CentOS7) \tristan{Improve contrast}. An animated version of the comparison is available 
    \href{https://github.com/big-data-lab-team/HCP-reproducibility-paper/blob/master/images/pfs_t2w_alignment.gif}
    {here}.
} 
    \label{fig:fnirt_result}
\end{figure}


\subsubsection{Between-OS differences in FreeSurfer} 

The only non-reproducible process identified by \toolname in FreeSurfer was
\texttt{mris\_make\_surfaces} (cortical and white matter surfaces
generation), which is also the only dynamically-linked executable in this pipeline
\tristan{Ali, could you check that this is indeed the only dynamic
executable in the pipeline?}. \texttt{mris\_make\_surfaces} produced different results for
10 out of 20 subjects. 

While by design \toolname only identifies processes that create
differences between conditions, pipeline results may still differ due to
the propagation and amplification of differences created at previous
processing steps, PreFreeSurfer in our case. We observed the effect of this
propagation in FreeSurfer results, as shown in
Figure~\ref{fig:tissue_class} for whole-brain segmentations. Furthermore,
the Dice coefficients associated with the 44 regions segmented by
FreeSurfer are shown in Figure~\ref{fig:scatter_plot}, showing low Dice
values (below 0.5) in the smallest structures.


\begin{figure}
%  \includegraphics{brain\_classification}
\centering
  \includegraphics[width=\columnwidth]{images/brain_segmentation_mni.png} 
  \caption{Sum of binarized differences between whole-brain FreeSurfer
  segmentations obtained from PreFreeSurfer processings in CentOS6 vs CentOS7
   (N=20). Segmentations were resampled and overlaid to the MNI152 volume
  template. An animated comparison of segmentations obtained for a particular subject is available
\href{https://github.com/big-data-lab-team/HCP-reproducibility-paper/blob/master/images/fs_brain_segmentation.gif}
{here}.
} 
  \label{fig:tissue_class}
\end{figure}

\begin{figure*}
  \centering
  \begin{subfigure}{0.45\linewidth}
    [FIGURE GOES HERE]
    \caption{Regions with \emph{less} than 10,000 voxels.}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.45\linewidth}
    [FIGURE GOES HERE]
    \caption{Regions with \emph{more} than 10,000 voxels.}
  \end{subfigure}
    % \includegraphics[width=\textwidth]{images/scatter_plot.png} 
    \caption{Dice coefficients between regions segmented by FreeSurfer in CentOS6 vs CentOS7 (N=20).
        To improve readability, the two graphs are on different y scales. \tristan{Ali, could you separate the figures in 2 and insert them in each subfigure?}} 
    \label{fig:scatter_plot}
  \end{figure*}

  % We formulated the linear regression model for the Dice and region size variables as:
  % \[D=1 + T\epsilon ;    T=\frac{-1}{\overline{R}}\]
  % It is easier to obtain high overlaps with large regions, 
  % while smaller regions are harder to be similarly segmented.
  % We obtained the epsilon value of $\approx$ 8.3 voxels ($\approx$ 3mm) which indicate the minimum number
  % of voxels in each region that needs to be segmented.
  

\section{Discussion}

\subsection{Key findings}

Linear and non-linear registration with FSL were found to
frequently lead to differences between results obtained with different
operating systems. This does not come as a surprise given the instabilities
associated with these processes. It also corroborates our previous findings
in~\cite{Glatard2015} where fMRI pre-processing using FSL was found to be
non-reproducible across operating systems starting from the motion
correction step, a step that uses FSL's \texttt{flirt} tool internally. It
would be relevant to investigate if the observed instability of
registration processes generalizes to other toolkits, or if it is specific
to FSL. In view of the effect of small data perturbations in a variety of
toolboxes and processes, such as cortical surface extraction using
FreeSurfer and CIVET~\cite{Lewis2017-ll} or connectome estimation using
Dipy~\cite{kiar2019comparing}, it is possible that this observation
generalizes widely across toolboxes and requires a deeper investigation of
the stability of linear and non-linear registration.

While only a handful or processes were found non-reproducible across the
tested operating systems, including most notably linear and non-linear
registration, the effect of such instabilities were found to propagate
widely in the pipelines, resulting in Dice coefficients frequently below
0.9 for the amygdala or ventricles. This illustrates the need to conduct such 
reproducibility studies for entire pipelines rather than isolated processes.

Furthermore, as shown in Figure~\ref{fig:pfs_freq}, the reproducibility of
a given tool may vary across subjects and across processing parameters. For
instance, linear registration with FLIRT seems to be fully reproducible in
the Anatomical Average sub-pipeline, while it is highly non-reproducible in
ACPC Alignment. In Brain Extraction, the same tool was found reproducible
for some subjects only. Therefore, reproducibility studies need to be
performed on several representative subjects rather than only a couple of
them. While this is common practice in neurosciences, it is common that
software tests be executed only on a single dataset, to reduce the
associated computational load. It is likely, however, that the observed
instabilities stem from numerical edge cases that may not manifest in all
datasets. 

While our findings give insight on the reasons why image analysis processes
are unstable across operating systems, they do not provide a complete
uncertainty quantification for neuroimaging pipelines across operating
systems. Although CentOS is a popular distribution in
neurosciences~\cite{hanke2011neuroscience}, different observations could be
made on other operating systems. The numerical noise introduced by
operating system updates is a realistic one, as such updates are likely to
occur throughout the time span of a neuroscience study, but it is also
uncontrolled, as it originates in updates of low-level libraries by
third-party developers. A possible method to study this problem more
comprehensively would be to introduce controlled numerical perturbations in
pipelines, which could be done either using data noise, or through
computational noise introduced by Monte-Carlo
Arithmetic~\cite{Parker1997-qq}. The work in~\cite{kiar2019comparing}
discusses and compares these two techniques.

\subsection{\toolname evaluation}

The processes identified by \toolname as non-reproducible were all
associated with dynamically-linked executables \tristan{Ali, please check
that}. This makes complete sense as statically-linked executables wouldn't
be impacted by library updates. Moreover, the hypothetical effects of
hardware or Linux kernel updates were not measured, as the different
operating sytems were deployed in Docker containers on the same host, that
is, using the same kernel and hardware.

To evaluate the reproducibility of a pipeline, \toolname needs to execute
it 3 times in order to (1) record a first \reprozip trace, (2) save
transient files, and (3) compare results in the second condition. This is
one more execution than the theoretical minimum of 2. It might be possible
to further reduce this overhead, by executing at step (2) only the
processes depending on transient files.

We demonstrated the applicability of our approach by evaluating two of the
arguably most complex pipelines in neuroimaging. Technically, these
pipelines consist of a mix of tools assembled from different toolboxes
through a variety of scripts written in different languages. Our file-based
approach, notably enabled by \reprozip, was able to analyze these pipelines
without requiring their instrumentation, which saved a very substantial
technical effort. The assumptions made on the pipeline structure, related
to the absence of concurrent writes, were not violated in our analysis, and
are likely to not impede \toolname's applicability to the most common
neuroimaging pipelines. 

However, file-based analyses also have limitations related to the
granularity at which they operate. Indeed, differences can only be
identified at the level of an entire operating-system process, which can
correspond to arbitrary amounts of code. Narrowing down the analysis to
particular libraries, functions, or even code sections would require
another approach. Similarly, \toolname would not be able to detect
differences in data not saved in files, and passed to subsequent processes
in memory. A common scenario in neuroimaging pipelines is that tools return
results in their standard output, which is parsed by the calling process
and passed to subsequent ones through variables. Differences that occur in data 
that is processed in this way would not be detected by \toolname.  

\section{Conclusion}

Summary of key findings. 

Spot works well but is a first step toward deeper investigation of 
particular processes.

\section{Acknowledgments}

This research was enabled in part by support provided by 
Calcul Quebec (http://www.calculquebec.ca) and 
Compute Canada (http://www.computecanada.ca).
Also, data were provided by the Human Connectome Project, WU-Minn 
Consortium (Principal Investigators: David Van Essen and Kamil Ugurbil; 
1U54MH091657) funded by the 16 NIH Institutes and Centers that support 
the NIH Blueprint for Neuroscience Research; and by the McDonnell 
Center for Systems Neuroscience at Washington University.


% \begin{figure*}
% \centering
%   \includegraphics[width=.9\textwidth]{images/graph}
%   \caption{A complete process graph from the PreFreeSurfer pipeline.
% Full-resolution image available at \url{https://drive.google.com/open?id=174yyn8SuVOUcK5aRVw0bagjDanLD0FLt}.}
%   \label{fig:complete-graph}
% \end{figure*}

% \begin{figure*}
% \centering
%   \includegraphics[width=.8\textwidth]{images/hclusters}
%   \caption{Different data types clustered among 100 subjects.}
%   \label{fig:subj-clusters}
% \end{figure*}



\bibliographystyle{plain}
\bibliography{biblio}


\end{document}
