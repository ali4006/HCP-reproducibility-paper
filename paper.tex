\documentclass{article}

\usepackage{pdfcomment}
\usepackage[margin=1in]{geometry}

\newcommand{\note}[2]{\pdfmargincomment[color=yellow,author=#1,open=true]{#2}}
\newcommand{\todo}[2]{\pdfmargincomment[color=red,author=#1,open=true]{#2}}

\title{Reproducibility of HCP pipelines across operating systems}

\author{Lalet Scaria, Lindsay B. Lewis, Alan C. Evans, Tristan Glatard}

\begin{document}

\maketitle

\abstract{This paper is a reproducibility study of the work in~\cite{glasser2015multi}.}

\section{Introduction}

Groenschild \emph{et al.} first identified the effect of operating
systems on Freesurfer
results~\cite{Gronenschild2012}. In~\cite{10.3389/fninf.2015.00012} we
quantified this effect on some of the main neuroimaging pipelines
including several FSL pipelines, CIVET and Freesurfer. In this work we
aim at evaluating this effect on the work in~\cite{glasser2015multi}.

The operating system is defined here as a consistent set of software
packages organized in a trusted repository.

The operating system is not the only part of the computational
environment that may hamper reproducibility. The work
in~\cite{diethelm2012limits} mentions reproducibility issues coming
from parallelization.

Reproducibility has several other aspects, see, e.g.,
\url{https://medium.com/@lorenaabarba/barba-group-reproducibility-syllabus-e3757ee635cf#.ty3zmgd4k}

For an overview of variability accross analysis methods in fMRI, see \url{http://journal.frontiersin.org/article/10.3389/fnins.2012.00149/full}.

For a general overview in fMRI, see \url{http://www.nature.com/nrn/journal/vaop/ncurrent/box/nrn.2016.167_BX3.html}

\section{Materials and Methods}

\subsection{Dataset}

Data: report on versions and subjects used.

\subsection{Minimal pre-processing pipelines}

Described in~\cite{glasser2013minimal}.

We installed the pipelines in three versions of Linux using Docker
containers. Refer to (immutable) version of containers used. Make sure
the exact same containers are used for all subjects (no rebuild as
rebuild would trigger package update). For each execution, we
monitored the list of packages used with versions.

We integrated the pipelines in CBRAIN through the Boutiques system.

\paragraph{Hardware.} We executed the pipelines on Compute Canada
using the Guillimin cluster of Calcul Qu\'ebec. We monitored the
hardware used (CPU and memory). 

\paragraph{FSL installation.} We developed our own FSL installer to be
able to customize easily the FSL version and build (the script
provided by FSL installs the latest version by default). We used FSL
5.0.6, built for CentOS5 and CentOS6.

\paragraph{Execution.} The pipeline was launched twice in the same
environment to detect any use of pseudo-random numbers. We
made sure that the results produced by these two executions were
identical.

\subsection{Results comparisons}

Describe the scripts that will be written to compare results.


Safeguards:
\begin{itemize}
\item Checksums to detect file corruption.
\item Each task works on its own copy of the input files to avoid race conditions in case pipeline modifies its inputs.
\item Hardware information is captured to make sure that differences are not coming from, e.g., different CPUs (when on cluster).
\item Checksum of Docker container is recorded to make sure that the same container was used all over.
\item Multiple runs are executed per condition to detect potential intra-condition differences (e.g., use of pseudo-random numbers).
\end{itemize}

Describe how metrics (distances?) are associated to file types (based on regexp) and used to compute distance matrices.


\section{Results}

Show lists of packages used with version. Focus on important
differences (e.g.: python interpreter, gcc if relevant) and explain
why these packages are important.

Show results of the comparison.

\section{Discussion}

Discuss the results.

\section{Conclusion}

Conclude and highlight future work.

\section{Acknowledgments}

CBRAIN team. Compute Canada.

\bibliographystyle{plain}
\bibliography{biblio}

\end{document}
