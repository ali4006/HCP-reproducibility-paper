% GigaScience template
\documentclass[a4paper,num-refs]{oup-contemporary}

\journal{gigascience}


%%%% Packages %%%%
\usepackage{siunitx}
\usepackage{minted} % Used for JSON highlighting
\usepackage{algpseudocode} % Algorithmic environment
\usepackage{xspace}
\usepackage{booktabs}

%%%%%%

\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{verbatim}
\usepackage{makecell}
\usepackage[flushleft]{threeparttable}

\usepackage{subcaption}
\usepackage{xspace}
\usepackage{stmaryrd} % for llbracket and rrbracket
\usepackage{amsmath}
\usepackage{ulem}
\usepackage{pifont}

\hypersetup{
colorlinks=true,% hyperlinks will be coloured
linkcolor=blue,% hyperlink text will be blue
linkbordercolor=blue,% hyperlink border will be blue
}
\makeatletter
\Hy@AtBeginDocument{%
  \def\@pdfborder{0 0 1}% Overrides border definition set with colorlinks=true
  \def\@pdfborderstyle{/S/U/W 1}% Overrides border style set with colorlinks=true
                                % Hyperlink border style will be underline of width 1pt
}
\makeatother

\DeclareMathOperator*{\argmin}{argmin}



%%%% Commands %%%%
\newcommand{\todo}[1]{\color{red}\textbf{TODO:}#1\color{black}}
\newcommand{\note}[2]{\color{blue}Note: #1\color{black}}
\newcommand{\reprozip}[0]{ReproZip\xspace}
\newcommand{\tristan}[1]{\color{blue}\textbf{From Tristan:}#1\color{black}}
\newcommand{\english}[1]{\uwave{#1}}
\newcommand{\toolname}[0]{Spot\xspace}
 
\title{File-based localization of numerical perturbations in data analysis pipelines}
\begin{document}

\author[1]{Ali Salari}
\author[2,3]{Gregory Kiar}
\author[2]{Lindsay Lewis}
\author[2,3]{Alan C. Evans}
\author[1]{Tristan Glatard}

\affil[1]{Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada}
\affil[2]{McGill University, Montreal, Canada}
\affil[3]{Montreal Neurological Institute, Montreal, Canada}



\maketitle

\begin{abstract} 

Data analysis pipelines are known to be impacted by operating system
updates and other infrastructural factors, presumably due to the creation,
propagation and amplification of numerical errors. However, the precise
causes of such instabilities and the path along which they propagate in the
pipelines are unclear, while this process could play a major role in the
current reproducibility crisis. We present a technique to identify the
computational processes in a pipeline that create numerical differences
when executed in different computational conditions. Our technique
leverages system-call interception to reconstruct processing and file
graphs without pipeline instrumentation. By applying our tool to the
structural pre-processing pipelines released by the Human Connectome
Project, we find that linear and non-linear registration are the cause of
most numerical instabilities in such pipelines, which confirms previous
findings. 

\end{abstract}

\begin{keywords}
Computational reproducibility; Numerical instability; Neuroimaging.
\end{keywords}


\section{Introduction}

% Reproducibility: crisis, numerical stability
% Containerization isn't the solution

Numerical stability is believed to play a central role in the differences
observed in data analysis pipelines across computational infrastructures.
Variability resulting from operating system upgrades, parallelization, or
even minor data perturbations may originate in errors that accumulate in
numerical computations, with important consequences on the reproducibility
of data analysis results. However, it remains challenging to identify the
exact elements in a complex pipeline that are prone to numerical instability,
leaving pipeline developers and users with the daunting task to manually
compare results obtained in different conditions. 

In neuroimaging, our primary application field, data analyses often consist
of hundreds of computational processes, sometimes coming from multiple
toolboxes, that are aggregated to perform a specific function. For
instance, the fMRIprep pipeline~\cite{esteban2019fmriprep} assembles software blocks
from FSL~\cite{jenkinson2012fsl}, AFNI~\cite{cox2012afni}, Freesurfer~\cite{fischl2012freesurfer} and ANTs~\cite{avants2009advanced} to provide a state-of-the art
functional MRI processing tool with minimal user input. Another example are the pipelines of the Human
Connectome Project~\cite{glasser2013} that combine tools from FSL and Freesurfer to pre-process
structural, functional and diffusion data from their
open dataset. In both cases, pipelines leverage toolboxes that are
widely trusted in the community. At the same time, substantial results
variations have been observed in these toolboxes resulting from minor data
or infrastructure perturbations~\cite{Glatard2015, Gronenschild2012, Lewis2017-ll}, which suggests that further investigation of their
numerical conditioning is required. For such complex pipelines, a
lightweight solution has to be found to perform such evaluations with
limited code instrumentation.

Numerical evaluations are traditionnally performed using techniques such as
interval arithmetics~\cite{hickey2001interval} that require complete code re-writes and are
therefore barely applicable to complex pipelines. Recently, Monte-Carlo
Arithmetic~\cite{Parker1997-qq, Denis2016-wo} provided a practical way to
evaluate the uncertainty of numerical results without the need to rewrite
the application in a different paradigm. By perturbating floating-point
computations, it introduces a controllable amount of noise in the
pipelines, effectively sampling results from a random distribution. While
this technique is very appealing, it suffers from two main issues that make
it impractical at the scale of a complete pipeline. First, it requires that
all software components be recompiled for MCA instrumentation, which is not
always feasible. Second, it multiplies the execution time by a factor of 10
to 100, which is impractical when executions already take a few hours to
complete. 

We present Spot, a tool to identify the source of numerical differences in
complex pipelines without instrumentation. Using system-call interception
through the Reprozip tool~\cite{chirigati2016reprozip}, Spot traverses graphs of processes and
intermediary files to pinpoint the pipeline components that are unstable
across execution conditions. When differences start accumulating,
effectively masking any further instability, it restores clean data copies
through a set of wrapper scripts. Wrapper scripts are also used to restore
temporary data that might have been deleted during the execution, and to
disambiguate files that have been written by multiple processes. The remainder of this paper 
presents the design of our tool, and its 
application to pre-processing pipelines of the HCP project. 

\todo{explain the issue with propagating differences and with transient files}

% % Verificarlo and MCA, require recompilation
% Numerical instabilities originate in the limited \todo{precision} of floating-point representations. 
% Traditionnaly, it has been evaluated through 

% % -> File-based analysis of pipelines without instrumentation
% % Reprozip

% Reproducibility is a crucial element of the scientific works, 
% as it enables researchers to evaluate authenticity and reliability 
% of the findings~\cite{plesser2018reproducibility}.
% Reproducibility is defined as the ability to regenerate the same 
% results as the original findings when the experiment is reanalyzed by 
% the same analytic methods, software package, parameters, and 
% data~\cite{peng2011reproducible}. 
% In addition, numerical reproducibility is defined as the ability to 
% regenerate bit for bit identical results from multiple 
% runs~\cite{hill2017numerical}. 
% We considered the numerical reproducibility in our experiments by comparing 
% the binary content of the results using the checksum method.

% Recently, validation of the reproducibility has been widely investigated 
% in the field of neuroimaging in which uses optimized 
% processing methods with the purpose of functional and structural 
% assessments of the human brain.
% According to the previous 
% studies, the variety of computing infrastructures including workstation 
% types, parallelization methods, operating systems, and analysis 
% packages are known to influence the reproducibility of the analyses~\cite{Gronenschild2012, 
% diethelm2012limits, Glatard2015, bowring2019exploring}.
% These irreproducibility issues are reported as the result of the 
% creation, propagation, and amplification of small numerical 
% differences.

% In particular, the effect of the operating systems on  
% computational pipelines shows the creation of small numerical differences~\cite{Glatard2015, Scaria2017}.
% These differences mainly correspond to the mathematical functions implemented 
% in different operating system libraries.
% For instance, changing the mathematical functions like \emph{expf()} and 
% \emph{cosf()} which manipulate the precision of floating-point representations, 
% between \emph{glibc} libraries in different operating systems can produce 
% small numerical differences.
% Besides, a similar issue is expected for any operating system which is 
% based on \emph{glibc}, the GNU C library.

% There are different approaches to improve the reproducibility of the analysis, 
% including containerization techniques that encapsulate software/hardware dependencies, 
% provenance capturing tools, and version control systems. 
% However, a comprehensive solution requires to fix the numerical instabilities instead of 
% masking the problem. For this purpose, bagging is one solution that has been used to 
% stabilize motion estimation analysis in fMRI~\cite{Glatard2018hbm}. 
% Bootstrap aggregation reduces the instabilities, but it needs expensive computations.
% As the second solution, a debugging tool can help to identify and 
% fix the instabilities in the pipeline. Therefore, we introduce 
% the numerical reproducibility measurement tool named \emph{Spot} to identify 
% the processes in the pipeline that create numerical differences across different conditions. 
% Furthermore, we can stabilize the identified processes in the next steps.
\todo{explain creation vs propagation}
\todo{define pipeline}

\section{Tool description}

\toolname identifies the processes in a pipeline that produce different
results when executed in two different conditions. First, a directed
bipartite provenance graph is recorded for each subject, where nodes
represent application processes and files, and edges represent read and
write file accesses (Step 1 in Figure~\ref{fig:overview-tool}). Second,
transient files, i.e., files that are either deleted during pipeline
execution or modified by multiple processes, are identified (Step 2) and
disambiguated, resulting in a provenance \emph{DAG} (Directed Acyclic
Graph) in which file nodes have a single parent (in-degree of 1) (Step 3)
\tristan{Ali, this is not true in Figure 1: at step 3 there are still
transient files}. DAGs produced in the two conditions are then compared, in
a step-by-step execution that prevents the propagation of differences in the pipeline (Step
4). Finally, a labeled graph is produced that identifies the
non-reproducible processes in the pipeline. 


To ensure that a file can be unambiguously associated with the process that
created it, we assume that pipeline processes can be re-ordered such that:
\begin{enumerate}
\item Processes don't run concurrently;
\item Each process sequentially reads, computes, and writes. 
\end{enumerate}
In practice, pipeline processes may still run concurrently provided that
they don't write concurrently to the same files. A process may also
interleave file writes with computing, for instance when different file
blocks are processed sequentially. However, only a single version of the
file must eventually be made available to the other processes. Similarly,
in case processes write and delete temporary files throughout their
execution, these temporary files must not be read by any other process in
the pipeline.

In addition, we also require that processes are
associated to a command line (executable and arguments), to facilitate
process instrumentation.

\begin{figure*}
  \centering
    \includegraphics[width=.8\textwidth]{images/spot-diagram}
    \caption{\toolname overview.}
    \label{fig:overview-tool}
  \end{figure*}

\subsection{Recording provenance graphs}

We use the \reprozip tool~\cite{rampin2016reprozip}
to capture: (1) the set of processes created by the
pipeline, and
(2) the set of files read and written by each process, including
temporary files. \reprozip collects this information through the
\texttt{ptrace()} system call, with no required instrumentation of the pipeline.
Using the \reprozip trace, \toolname reconstructs a provenance graph by creating process and file
nodes and by adding directed edges corresponding
to file reads and writes (Figure~\ref{fig:provenance-graph}).

Provenance graphs are often subject-dependent, due to variations in input data among subjects.
%, which justifies the processing
% of multiple subjects. For instance, data acquisitions may be
% repeated a different number of times in each subject, for noise-reduction
% purposes. 
Some of these differences can be neglected, for instance when a
data decompression step is present at the beginning of the execution for
some subjects only, but other differences cannot, for instance
when complete different processing paths are used in different subjects. \toolname 
includes helpers to identify different instances of provenance graphs \tristan{mention tree distance here}.

\begin{listing}
  \inputminted{bash}{"bin/example.sh"}
  \caption{Example pipeline}
  \label{listing:sample-script}
\end{listing}

\begin{figure}
\begin{subfigure}{0.4\linewidth}
  [ FIGURE GOES HERE ]

  \caption{Raw provenance graph (\reprozip output), with transient files shown in gray.}
  \label{fig:provenance-graph}
\end{subfigure}
\hfill
\begin{subfigure}{0.4\linewidth}
  [ FIGURE GOES HERE ]

  \caption{Provenance DAG, with disambiguated transient files.}
  \label{fig:provenance-dag}
\end{subfigure}
  % \includegraphics[width=.8\columnwidth]{images/provenance-graphs}
  \caption{Provenance graphs
  created from the example pipeline in
  Listing~\ref{listing:sample-script}.
  Processes are represented with circles, files with rectangles, and read/write accesses 
  with plain edges. 
  For visualization purposes, the process tree is represented too, with
  dashed edges. Every node in the graph is labeled using (1) a process id
  created by our reconstruction, (2) the name of the executable run by the
  process or the file name. Process 0 is the initial call to the
  \texttt{sh} interpreter; the other processes are the calls to FSL bet,
  stats and maths made in Listing~\ref{listing:sample-script}. Process 73
  is forked by process 2: it was captured by \reprozip while it did not
  appear in Listing~\ref{listing:sample-script}.
}
  \label{fig:simple_script}
\end{figure}

\subsection{Capturing transient files}

We capture temporary files by replacing every
process P by a wrapper that first calls P and then saves the produced
temporary files to a read-only directory. This process replacement is done by pre-pending
 to the \texttt{PATH} environment
variable a directory that contains a wrapper script named after the executable
called by P.

Files written by multiple processes are disambiguated using a similar technique. For a
 file F written by the processes in \textbf{P} = \{$P_{1}$, \ldots,
 $P_{n}$\}, we first check that processes in \textbf{P} do not
 write concurrently to F, which would violate our assumptions. Then, we
 replace every process $P_{i}$ by a \texttt{PATH}-based wrapper that first
 calls $P_{i}$ and then saves F to a read-only directory. In this way,
 successive versions of F are preserved for comparison. We finally
 update the provenance graph accordingly, so that all files in the graph
 have an in-degree of 1 (Figure~\ref{fig:provenance-dag}). This operation also makes the provenance graph
 acyclic, since we assumed that a process could only release a single version of a file.



\subsection{Labelling processes} 

After capturing transient files, we re-run the pipeline
step by step in Condition 2 to label processes. The output files
produced by a process in Condition 2 are compared to the ones produced
in Condition 1. If no differences are found, the process is marked as
reproducible. Otherwise, the process is marked as non-reproducible and the
output files produced in Condition 1 are copied to Condition 2, to ensure
that differences do not propagate further in the pipeline. Processes are
instrumented transparently, through a modification of the \texttt{PATH}
variable similar to the one described previously. By default, differences
in output files are identified by comparing file checksums. Other
comparison functions can also be defined for specific file types, for
instance to ignore file headers or file sections containing timestamps.
\toolname finally creates a labeled
provenance graph highlighting non-reproducible processes.

Figure~\ref{fig:iterations} illustrates the incremental labelling 
process for the example in Figure~\ref{fig:simple_script}. 
At step 1, process \texttt{73\#bet2} is labeled as non-reproducible (red) 
as it produces files with differences. Subsequently, the files produced by \texttt{73\#bet2} in  
Condition 2 are replaced with the files produced by \texttt{73\#bet2} in 
Condition 1.
At step 2, process \texttt{75\#fslmaths} is executed and then labeled 
as reproducible (green) as it produced files without differences.
At step 3, \texttt{76\#fslstats} is also labeled as 
reproducible as it didn't produce any file.

\begin{figure}
  \centering
  \includegraphics[width=.8\columnwidth]{images/labelling-process}
  \caption{Illustration of the labelling approach. Non-reproducible processes are shown in red.}
  \label{fig:iterations}
\end{figure}



\section{Experiments}

We applied the \toolname to find the origin of between-OS differences that
frequently occur in neuroimaging pipelines. In particular, we studied the minimal
pre-processing pipelines released by the Human Connectome Project
(\href{https://www.humanconnectome.org}{HCP}), a leading initiative in
neurosciences. 

% In this experiment, we analyse the numerical reproducibility of computational pipelines 
% and identify the origin of differences across the operating systems. 
% Two types of differences can occur in the subjects due to the differences
% in the operating systems. One is between-OS differences caused by the
% operating system library updates and the other type, within-OS differences
% occur as a result of the pseudo-random processes used in the pipelines.

% In particular, Spot tool is tested on the neuroimaging applications which are 
% predominantly using mathematical libraries. Therefore, we expect to find 
% differences as a result of changing the mathematical functions between operating system libraries.

% This section describes datasets and pipelines used for the analyses, and the way of data processing.

\subsection{HCP pipelines and dataset}

% We used our tool to evaluate the reproducibility of the pipelines from the Human Connectome 
% Project .
% The HCP initiative is an effort to acquire and analyse 
% brain connectivity data from 1200 healthy adults.
% It enables the neuroscience 
% research community to discover relationships between brain circuits and 
% individual behaviors. This helps to understand a wide range of brain disorders.
% The HCP project provides database services (ConnectomeDB) for storing and 
% sharing primary and processed data freely, and data analysis pipelines that 
% are available under an open-source license.

The HCP developed a set of pre-processing pipelines to process structural,
functional, and diffusion MRI data acquired in the project. We focus on HCP
pre-processing pipelines for structural data, and particularly
on PreFreeSurfer and FreeSurfer. 
A detailed description of the analyses done by these
pipelines is available in~\cite{glasser2013}. 
In summary, PreFreeSurfer consists of the following steps: 
\begin{itemize}
\item Distortion Correction (DC), 
\item Anatomical Average (AAve), 
\item Anterior/Posterior Commissure Alignment (ACPC-A), 
\item Brain Extraction (BExt), 
\item Bias Field Correction (BFC), 
\item Atlas-Registration (AR).
\end{itemize}
And FreeSurfer consists of the following ones: \tristan{Check these steps}
\begin{itemize}
\item Image downsampling, 
\item T1w image registration, 
\item Surface placement, 
\item Surface registration.
\end{itemize}
The average processing time per subject is approximately 2~hours for
PreFreeSurfer and 8~hours for FreeSurfer. The average output file size is
approximately 2.7~GB for PreFreeSurfer and 4.1~GB for FreeSurfer.
%\tristan{Ali, are these numbers per subject or for 20 subjects? it's an average of 20 subjects}

We randomly selected 20 unprocessed subjects 
from the HCP data release S500 
available in \href{https://db.humanconnectome.org}{the ConnectomDB repository}. 
For each subject, available data consisted of 1 or 2 T1 images, and 1 or 2 T2 images. 
% \tristan{Can you let me know where the data is located, so that I could describe it here?
% Subjects are located in consider:/data/asalari/ali-tests/PFS\_Centos6\_Traced
% and consider:/data/asalari/ali-tests/PFS\_Centos7\_Traced 
% and outputs are in consider:/data/asalari/ali-tests/nurm-out}
Acquisition protocols and parameters are detailed in~\cite{van2013wu}. \tristan{Add summary of matrix sizes.}

% HCP data collected from different types of imaging techniques, including 
% structural imaging (sMRI), functional imaging (fMRI), and diffusion imaging (dMRI).
% The structural images include T1-weighted (T1w) and T2-weighted (T2w) images, which 
% help in the diagnosis of brain injury.
% The images are indexed with a suffix if several scans of the same modality were acquired.
% For example, some data may include two T1w images with low and high resolutions.
% The functional images, including task-based fMRI and resting-state scans, 
% enable the measurement of functional activations within brain areas. 
% Diffusion imaging is another kind of MRI technique, which measures 
% the anatomical connectivity between regions.

\subsection{Data processing}

We built Docker images for the HCP pre-processing pipelines v3.19.0
(PreFreeSurfer and FreeSurfer) in CentOS 6.9 (Final) and CentOS 7.4 (Core), available on
\href{https://hub.docker.com/r/bigdatalabteam/hcp-prefreesurfer/}{DockerHub}.
Container images contain the HCP software dependencies, including FSL
(version 5.0.6), FreeSurfer (version 5.3.0-HCP, CentOS4 build), and
Connectome Workbench (version 1.0).

We processed the 20 subjects with PreFreeSurfer and FreeSurfer, using the 2
CentOS versions. Each subject was processed twice on the same operating
system to detect within-OS variability coming from pseudo-random
operations. We ignored execution-specific information such as file path or
timestamps by using FreeSurfer tools \texttt{mri\_diff},
\texttt{mris\_diff}, and \texttt{lta\_diff} that determine if MRI volumes
or transformations differ. To compare segmentations, we used the Dice coefficient defined as follows:
\[DICE=\frac{2|X \cap Y|}{|X| + |Y|}\].

% Finally, to cluster the subjects, the threshold value is set to zero in the clustering method. 
% Also, we used the nearest neighbor algorithm to calculate the distance between clusters.


\subsection{Results}

% \subsection{Subject clustering}

% To reduce the number of provenance graphs to be examined, we cluster
% process trees using agglomerative hierarchical clustering, as implemented
% in SciPy~\cite{oliphant2007scipy}. We use the tree edit
% distance~\cite{zhang1989simple} between process trees, as implemented in
% the zss Python package. This distance is defined as the minimum number of
% edit operations to transform one tree into the other. Three edit operations
% are considered: node label modification, node removal, and node insertion.
% Each operation has an associated cost of 1. 
% //If threshold is 0, why do we even need the edit distance?//


\subsubsection{Within-OS differences}

We did not observe any within-OS difference in PreFreeSurfer. In
FreeSurfer, we identified 2 processes leading to within-OS differences due
to the use of pseudo-random numbers: image registration with
\texttt{mri\_segreg}, and cortical surface curvature estimations with
\texttt{mris\_curvature}. To remove these differences, we fixed the random
seed used by FreeSurfer using the \texttt{--seed} command-line option. 

\subsubsection{Between-OS differences in PreFreeSurfer}
%\subsubsection{PreFreeSurfer pipeline analysis} 

We identified four types of subjects with different PreFreeSurfer
provenance graphs (Table~\ref{table:data-clusters}). Differences between
subject types are coming from different  numbers of T1 and T2 images in the
raw data: subjects of type 1 have 2 T1 images and 2 T2 images, while
subjects of type 2 only have 1 T1 image and 1 T2 image. We verified that
the provenance graphs were identical for all subjects of the same type, for
both versions of CentOS.

\begin{table}
\centering
\begin{threeparttable}
\caption{Types of provenance graphs in PreFreeSurfer.}
\label{table:data-clusters}

\begin{tabular}{cccccc}
\toprule
       &                        &  \multicolumn{4}{c}{Image modalities in subject data}    \\ 
\cmidrule(lr){3-6}       
Type   &   \makecell{Number of \\ Subjects}   &  T1w\_1          & T1w\_2      & T2w\_1          & T2w\_2        \\ \midrule
1      &               9                      &   \ding{51}      &   \ding{51} &   \ding{51}     &   \ding{51}   \\ 
2      &               8                      &   \ding{51}      &             &   \ding{51}     &               \\ 
3      &               1                      &   \ding{51}      &             &   \ding{51}     &   \ding{51}   \\ 
4      &               2                      &   \ding{51}      &   \ding{51} &   \ding{51}     &               \\ 
\bottomrule
\end{tabular}
\end{threeparttable}
\end{table}


Figure~\ref{fig:pfs_freq} shows the frequency of non-reproducible pipeline processes
in PreFreeSurfer. 
Differences were observed in linear registration 
with FSL FLIRT (in ACPC-Alignment, Brain Extraction, Distortion Correction, and
Atlas Registration), in non-linear registration with FSL FNIRT (in Brain Extraction 
and Atlas Registration), and in image warping with FSL \texttt{new\_invwarp} (in Brain Extraction 
and Atlas Registration). Differences were also observed in image mean 
computations with FSL maths  (in Anatomical Average). \tristan{figure and caption to be fixed.} 


% Between subject differences can be a result of different subject types. 
% For instance, the subjects contain two T1 images need more calculations 
% to produce an average image between them.


% make figure in two columns use {figure*}
\begin{figure*}
\centering
  \includegraphics[width=\textwidth]{images/pfs_heatmap.png}
  \caption{Heatmap of non-reproducible processes across PreFreeSurfer pipeline steps (N=20). 
  \tristan{Remove frequency values as they are shown by the color already. Remove multipliers, multiply cells instead. Cells should just contain subject numbers.}
  Each cell represents the occurrence of a particular command line in a
  pipeline step among Anatomical Average (AAve), Anterior/Posterior
  Commissure Alignment (ACPC-A), Brain Extraction (BExt), Bias Field
  Correction (BFC), or Atlas-Registration (AR). Cell labels indicate the
  number of subjects where the corresponding process appears. For example,
  the FLIRT tool was invoked 6 times in step DC for each of the 20
  subjects: 2 instances weren't reproducible in 95\% of the subjects, 3
  instances were always reproducible, and 1 instance wasn't reproducible in
  85\% of the subjects.
        }
  \label{fig:pfs_freq}
\end{figure*}


To visually illustrate the observed differences,
Figure~\ref{fig:fnirt_result} compares FNIRT results in Brain Extraction
for a particular subject. Differences appear to be very substantial, in
particular in the frontal and parietal lobes. 
\begin{figure}
  \centering
    \includegraphics[width=\columnwidth]{images/t2w_alignment.png} 
    \caption{Differences between T2 FNIRT results in PreFreeSurfer's Brain Extraction (CentOS6 vs 
    CentOS7) \tristan{Improve contrast}. An animated version of the comparison is available 
    \href{https://github.com/big-data-lab-team/HCP-reproducibility-paper/blob/master/images/pfs_t2w_alignment.gif}
    {here}.
} 
    \label{fig:fnirt_result}
\end{figure}


\subsubsection{Between-OS differences in FreeSurfer} 

The only non-reproducible process identified by \toolname in FreeSurfer was
\texttt{mris\_make\_surfaces} (cortical and white matter surfaces
generation), which is also the only dynamically-linked executable in this pipeline
\tristan{Ali, could you check that this is indeed the only dynamic
executable in the pipeline?}. \texttt{mris\_make\_surfaces} produced different results for
10 out of 20 subjects. 

While by design \toolname only identifies processes that create
differences between conditions, pipeline results may still differ due to
the propagation and amplification of differences created at previous
processing steps, PreFreeSurfer in our case. We observed the effect of this
propagation in FreeSurfer results, as shown in
Figure~\ref{fig:tissue_class} for whole-brain segmentations. Furthermore,
the Dice coefficients associated with the 44 regions segmented by
FreeSurfer are shown in Figure~\ref{fig:scatter_plot}, showing low Dice
values (below 0.5) in the smallest structures.


\begin{figure}
%  \includegraphics{brain\_classification}
\centering
  \includegraphics[width=\columnwidth]{images/brain_segmentation_mni.png} 
  \caption{Sum of binarized differences between whole-brain FreeSurfer
  segmentations obtained from PreFreeSurfer processings in CentOS6 vs CentOS7
   (N=20). Segmentations were resampled and overlaid to the MNI152 volume
  template. An animated comparison of segmentations obtained for a particular subject is available
\href{https://github.com/big-data-lab-team/HCP-reproducibility-paper/blob/master/images/fs_brain_segmentation.gif}
{here}.
} 
  \label{fig:tissue_class}
\end{figure}

\begin{figure*}
  \centering
  \begin{subfigure}{0.45\linewidth}
    [FIGURE GOES HERE]
    \caption{Regions with \emph{less} than 10,000 voxels.}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.45\linewidth}
    [FIGURE GOES HERE]
    \caption{Regions with \emph{more} than 10,000 voxels.}
  \end{subfigure}
    % \includegraphics[width=\textwidth]{images/scatter_plot.png} 
    \caption{Dice coefficients between regions segmented by FreeSurfer in CentOS6 vs CentOS7 (N=20).
        To improve readability, the two graphs are on different y scales. \tristan{Ali, could you separate the figures in 2 and insert them in each subfigure?}} 
    \label{fig:scatter_plot}
  \end{figure*}

  % We formulated the linear regression model for the Dice and region size variables as:
  % \[D=1 + T\epsilon ;    T=\frac{-1}{\overline{R}}\]
  % It is easier to obtain high overlaps with large regions, 
  % while smaller regions are harder to be similarly segmented.
  % We obtained the epsilon value of $\approx$ 8.3 voxels ($\approx$ 3mm) which indicate the minimum number
  % of voxels in each region that needs to be segmented.
  


\section{Discussion}

Static vs dynamic executables

The reproducibility of a given tool may vary across subjects, and across parameters. 
See FLIRT in DC in Figure 4.
Two types of differences are observed: between-process and between-subject.
Between-process differences occur when the same tool behaves differently in different process instances.
For example, \texttt{flirt}
always produces files without differences in the \texttt{AAve} sub-pipeline but 
almost always creates different files in the \texttt{AAli} sub-pipeline.
Conversely, the same process can be reproducible for some subjects but not for others. 



Propagation is very important. 

  The similar findings in another study~\cite{bowring2019exploring} were reported 
  the effect of between-subject differences on reproducibility.

* granularity is that
 of processes, no in-memory checks.

Method:
* we assume no concurrent file accesses.

* we assume that the provenance graph is oriented, i.e., a process cannot read or write in the same file. Reasonnable in general but wouldn't work 
for, say, out of core computations.

Comment on the overhead, number of executions (3, 2 in condition 1, 1 in condition 2)

Applicability: no instrumentation required, fast.
But not able to capture dependencies in memory.

We identified four processes that create differences 
among all 28 distinct processes that are involved in the PreFreeSurfer execution. 
The numerical instability in the 
PreFreeSurfer HCP pipeline arises mainly from linear 
registration processes implemented in FSL \emph{FLIRT}. 
We can see that \emph{FLIRT} creates differences in almost all the sub-pipelines. 
We monitored the library system calls using \emph{ltrace} to check that registration processes
use the same libraries. There were no differences in the library dependencies used by 
the tool in two operating systems, but we found a different number of calls to the \emph{glibc}.

In the second experiment, 
the analyses obtained by the FreeSurfer show substantial between-OS differences.
These results show the sensitivity of the segmentation process to the 
size of regions, however, it might be also sensitive to the location and shape of 
the segmented regions.
We demonstrate that static build of the pipeline would not improve the numerical stability of the pipeline. 
FreeSurfer pipeline propagates and amplifies the numerical differences that are created by the 
differences in the underlying operating system libraries.
These results are consistent with the findings in the paper~\cite{Glatard2015}, 
as it has confirmed the propagation of differences in cortical thickness extraction analysis using FreeSurfer.

We obtained results between OS versions; further investigations could be done over the OS families (e.g., Linux VS. Windows).
In addition to variations of the operating system, numerical instability of the pipelines have been 
explored when varying software, hardware and implementation details, or even by adding negligible 
perturbations like one voxel noise in the input dataset.
In the field of numerical analysis, we can investigate these issue using the Monte Carlo Arithmetic (MCA), 
a method instrumenting floating-point operations based on the random rounding algorithms at a target precision.
By this algorithm, we can locate the instabilities for the given pipelines, tools, or operations.

We demonstrate that computational models in neuroimaging are sensitive to numerical instabilities. 
Although we identify the processes in the pipeline that create differences, 
the amplification of these differences should be quantified in the future works.
We can use distance-preserving hashes rather than MD5 checksums or metrics specific to the file type like the Dice coefficient 
for the registration and segmentation processes to measure the magnitude of differences. 

% mention fuzzy libmath as a way to do this across distributions.

\section{Conclusion}

Our technique can characterize the stability of the pipelines
automatically. We identified the leading causes of the between-OS differences as (1) the evolution 
of the math libraries over time and (2) the instability of the pipelines. 
There are two ways to tackle this problem. The easy but less preferred solution is masking the instabilities.
This can be done by using, (1) single operating system for the processing of subjects, 
(2) containerizing the pipelines so that the 
processing is done on a more controlled environment, 
(3) increasing the numerical precision of the arithmetics, 
(4) building static executables by removing the host operating system library dependencies. 
These solutions only make the problem invisible, but the pipelines are still unstable.

\note{The preferred solution is to find the origin of amplification of differences and 
then fixing the particular functions/modules in the processing pipelines. 
Bagging is a fundamental technique toward reducing the instability of the analysis.
Therefore, we can use bagging for the parts in the pipeline that amplify differences, 
but it still is computationally expensive. }

% importance of data-driven evaluations, subject clustering, etc

% future work with MCA

% link registration to our motion estimation study

\section{Acknowledgments}

This research was enabled in part by support provided by 
Calcul Quebec (http://www.calculquebec.ca) and 
Compute Canada (http://www.computecanada.ca).
Also, data were provided by the Human Connectome Project, WU-Minn 
Consortium (Principal Investigators: David Van Essen and Kamil Ugurbil; 
1U54MH091657) funded by the 16 NIH Institutes and Centers that support 
the NIH Blueprint for Neuroscience Research; and by the McDonnell 
Center for Systems Neuroscience at Washington University.


% \begin{figure*}
% \centering
%   \includegraphics[width=.9\textwidth]{images/graph}
%   \caption{A complete process graph from the PreFreeSurfer pipeline.
% Full-resolution image available at \url{https://drive.google.com/open?id=174yyn8SuVOUcK5aRVw0bagjDanLD0FLt}.}
%   \label{fig:complete-graph}
% \end{figure*}

% \begin{figure*}
% \centering
%   \includegraphics[width=.8\textwidth]{images/hclusters}
%   \caption{Different data types clustered among 100 subjects.}
%   \label{fig:subj-clusters}
% \end{figure*}



\bibliographystyle{plain}
\bibliography{biblio}


\end{document}
